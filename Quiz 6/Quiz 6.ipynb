{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab74befe",
   "metadata": {},
   "source": [
    "### Q1 Querying Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef50b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 15:52:16 WARN Utils: Your hostname, macbook.local resolves to a loopback address: 127.0.0.1; using 10.243.54.6 instead (on interface en0)\n",
      "24/10/28 15:52:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/28 15:52:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark as spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import split, col, udf, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfdd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "+----+\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"1\",), (\"2\",)], [\"col1\"])\n",
    "\n",
    "# Cast the column to string type\n",
    "# df = df.withColumn(\"col1_str\", col(\"col1\").cast(\"string\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "# print(type(df.tail(1)[0]))\n",
    "print(df.tail(1)[0]['col1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800ba64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    user|  query|\n",
      "+--------+-------+\n",
      "|sndr0000|qry0045|\n",
      "|sndr0000|qry0049|\n",
      "|sndr0001|qry0048|\n",
      "|sndr0000|qry0055|\n",
      "|sndr0004|qry0014|\n",
      "|sndr0003|qry0038|\n",
      "|sndr0005|qry0024|\n",
      "|sndr0002|qry0039|\n",
      "|sndr0005|qry0011|\n",
      "|sndr0001|qry0041|\n",
      "|sndr0001|qry0001|\n",
      "|sndr0000|qry0040|\n",
      "|sndr0000|qry0035|\n",
      "|sndr0005|qry0012|\n",
      "|sndr0000|qry0040|\n",
      "|sndr0002|qry0035|\n",
      "|sndr0004|qry0029|\n",
      "|sndr0004|qry0043|\n",
      "|sndr0004|qry0034|\n",
      "|sndr0001|qry0026|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# queries contains about 300 data points, or roughly 5% of the total queries allowed\n",
    "queries = spark.read.text(\"queries.txt\")\n",
    "queries = queries.select(split(queries.value,\" \")).rdd.flatMap(lambda x: x).toDF(schema = ['a','b','c','d','user','e','f','query'])\n",
    "# drop useless columns.\n",
    "queries = queries.drop('a','b','c','d','e','f')\n",
    "queries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf4937",
   "metadata": {},
   "source": [
    "### finding number of duplicates for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8322f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sndr0000 duplicate queries\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        25|\n",
      "+----------+\n",
      "\n",
      "sndr0001 duplicate queries\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        21|\n",
      "+----------+\n",
      "\n",
      "sndr0002 duplicate queries\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        23|\n",
      "+----------+\n",
      "\n",
      "sndr0003 duplicate queries\n",
      "CodeCache: size=131072Kb used=30515Kb max_used=30526Kb free=100557Kb\n",
      " bounds [0x00000001069f8000, 0x00000001087f8000, 0x000000010e9f8000]\n",
      " total_blobs=11365 nmethods=10412 adapters=864\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        33|\n",
      "+----------+\n",
      "\n",
      "sndr0004 duplicate queries\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        30|\n",
      "+----------+\n",
      "\n",
      "sndr0005 duplicate queries\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        29|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = ['sndr0000','sndr0001','sndr0002','sndr0003','sndr0004','sndr0005']\n",
    "for u in users:\n",
    "    print(u,'duplicate queries')\n",
    "    s1 = queries.where(col(\"user\") == u)\n",
    "    s1 = s1.groupBy(s1.columns)\\\n",
    "        .count()\\\n",
    "        .where(col('count') > 1)\\\n",
    "        .select(F.sum('count'))\\\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9aff94",
   "metadata": {},
   "source": [
    "### calculating average number of duplicates by any user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d084697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.833333333333332\n"
     ]
    }
   ],
   "source": [
    "print((25+21+23+29+30+33)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4ce25",
   "metadata": {},
   "source": [
    "## Q2 Bloom Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838406f",
   "metadata": {},
   "source": [
    "### implementing the bloom filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d96a1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "import base64\n",
    "import math\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "\n",
    "class filter(object):\n",
    "\n",
    "    def __init__(self, items_count, size, fil):\n",
    "        \n",
    "        # Size of bit array to use\n",
    "        self.size = size\n",
    "        # number of hash functions to use\n",
    "        self.hash_count = self.get_hash_num(self.size, items_count)\n",
    "\n",
    "        # Bit array of given size\n",
    "        self.bit_array = fil\n",
    "\n",
    "    # check if item is in filter\n",
    "    def check(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            digest = mmh3.hash(item, i) % self.size\n",
    "            # if false bit detected, return false\n",
    "            if self.bit_array[digest] == False:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # return the number of hash functions needed. Number is calculated based on size of the array and number of input words\n",
    "    def get_hash_num(self, s, n):\n",
    "        k = (s/n) * math.log(2)\n",
    "        return int(k)\n",
    "\n",
    "def filter_negativity(word):\n",
    "    \n",
    "    with open('vector.txt', 'r') as file:\n",
    "        file_content = file.read()\n",
    "        file_content = base64.b64decode(file_content)\n",
    "\n",
    "    fil = bitarray(file_content.decode(\"utf-8\"))\n",
    "    bloom = filter(63, 1024, fil)\n",
    "    return bloom.check(word)\n",
    "# print(filter_negativity('happy'))\n",
    "# print(filter_negativity('damned'))\n",
    "negUDF = udf(lambda x:filter_negativity(x),StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c35223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# apply filter to each word\n",
    "negativity = words.select(col(\"word\"), negUDF(col(\"word\")).alias(\"nagative\") ).show()\n",
    "                          \n",
    "'''# Custom UDF with select()  \n",
    "df.select(col(\"Seqno\"), upperCaseUDF(col(\"Name\")).alias(\"Name\") ).show(truncate=False)'''\n",
    "\n",
    " # Start running the query that prints the words and their AFINN negativity aaffiliation to the console\n",
    "query = negativity \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9698e9",
   "metadata": {},
   "source": [
    "## Q3 Hyperloglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16584d44",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://www.statology.org/pyspark-count-duplicate-rows/\n",
    "\n",
    "[2] https://stackoverflow.com/questions/48554619/count-number-of-duplicate-rows-in-sparksql\n",
    "\n",
    "[3] https://www.statology.org/pyspark-count-duplicate-rows/\n",
    "\n",
    "[4] https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
